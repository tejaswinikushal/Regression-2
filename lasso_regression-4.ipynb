{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c733030-2543-448d-9a95-acf125864c75",
   "metadata": {},
   "source": [
    "#### Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274e5a2e-aa37-441f-8fa6-b9c18d4a1fd4",
   "metadata": {},
   "source": [
    "#### Lasso Regression:\n",
    "                 The goal of Lasso Regression is to minimize the sum of squared errors between the observed and predicted values, while also penalizing the absolute values of the coefficients.\n",
    "                 \n",
    " In Ridge Regression, the penalty term is based on the squared values of the coefficients, while in Lasso Regression, it is based on the absolute values.This leads to a sparsity effect in Lasso Regression, as it tends to drive some of the coefficients to exactly zero. As a result, Lasso Regression can be used for feature selection, effectively choosing a subset of important features and discarding the less important ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6660839e-ddab-48f0-9914-4b5494c6fb9f",
   "metadata": {},
   "source": [
    "#### Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21635ec0-2c6b-4ae9-b6d7-dac86939c3bd",
   "metadata": {},
   "source": [
    "In Lasso Regression, it is based on the absolute values. This leads to a sparsity effect in Lasso Regression, as it tends to drive some of the coefficients to exactly zero. As a result, Lasso Regression can be used for feature selection, effectively choosing a subset of important features and discarding the less important ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72d58fb-5eb0-42fa-a043-d8c2c2d4856a",
   "metadata": {},
   "source": [
    "#### Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5b2548-b64d-4f02-8787-2c9a369715d3",
   "metadata": {},
   "source": [
    "Interpreting the coefficients of a Lasso Regression model is similar to interpreting coefficients in a regular linear regression model, but with the added consideration that some coefficients may be exactly zero due to the sparsity-inducing nature of Lasso.\n",
    "\n",
    "#### Non-Zero Coefficients:\n",
    "\n",
    "If a coefficient is non-zero, its interpretation is similar to that in a standard linear regression model. It represents the change in the response variable for a one-unit change in the corresponding predictor, holding other predictors constant.\n",
    "\n",
    "#### Zero Coefficients:\n",
    "\n",
    "If a coefficient is exactly zero, it means that the corresponding predictor has been effectively excluded from the model. In other words, the Lasso algorithm has selected this feature for regularization, and it has no impact on the prediction. This feature can be considered as not contributing to the model.\n",
    "\n",
    "#### Magnitude of Coefficients:\n",
    "\n",
    "The magnitude of non-zero coefficients indicates the strength of the relationship between the predictor and the response variable. Larger magnitudes suggest a more substantial impact on the predicted outcome.\n",
    "\n",
    "#### Sign of Coefficients:\n",
    "\n",
    "The sign (positive or negative) of a non-zero coefficient indicates the direction of the relationship between the predictor and the response variable. A positive coefficient implies a positive association, while a negative coefficient implies a negative association.\n",
    "#### Feature Selection:\n",
    "\n",
    "One of the advantages of Lasso Regression is its ability to perform automatic feature selection by shrinking some coefficients to zero. Therefore, the non-zero coefficients can help identify the most important predictors in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b56ea0c-8398-44f0-bb37-0068e6081802",
   "metadata": {},
   "source": [
    "#### Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18646088-5ece-4167-8343-d075a0b6368a",
   "metadata": {},
   "source": [
    "In Lasso Regression, the main tuning parameter is the regularization parameter, λ\n",
    "\n",
    "#### Low λ:\n",
    "\n",
    "When λ is very close to zero, the penalty term has little effect. The model is similar to a standard linear regression, and it may include all features with non-zero coefficients. This can lead to overfitting, especially when dealing with a large number of features.\n",
    "\n",
    "#### Moderate λ:\n",
    "\n",
    "As λ increases, the penalty term becomes more influential, leading to a more significant shrinkage of coefficients. Some coefficients may be exactly zero, resulting in a sparser model. This helps in feature selection and prevents overfitting to the training data.\n",
    "#### High λ:\n",
    "\n",
    "When λ is very large, the penalty dominates the objective function. The model tends to have many coefficients set exactly to zero, resulting in a highly sparse model. This extreme regularization helps simplify the model further but may lead to underfitting if the penalty is too strong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed07590-5aee-40ec-80ce-0629be92d758",
   "metadata": {},
   "source": [
    "#### Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07faf19-dc44-4387-800b-8415e4800350",
   "metadata": {},
   "source": [
    "Yes,Lasso Regression be used for non-linear regression problems.\n",
    "\n",
    "#### Polynomial Features:\n",
    "\n",
    "One common approach is to introduce polynomial features. You can create polynomial features by raising the original features to higher powers. For example, if you have a single predictor x, you can include x^2,x^3, and so on as additional features. Then, apply Lasso Regression to the extended feature set. This allows the model to capture non-linear relationships.\n",
    "\n",
    "#### Interaction Terms:\n",
    "\n",
    "Another way to introduce non-linearity is by including interaction terms. Interaction terms capture the product of two or more features, allowing the model to account for interactions between predictors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d95ee3-7bb0-42c2-aceb-480bed7320b8",
   "metadata": {},
   "source": [
    "#### Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed7c028-a5ea-4b22-96ed-b850a2d9d46b",
   "metadata": {},
   "source": [
    "In Ridge Regression, the penalty term is based on the squared values of the coefficients, while in Lasso Regression, it is based on the absolute values.This leads to a sparsity effect in Lasso Regression, as it tends to drive some of the coefficients to exactly zero. As a result, Lasso Regression can be used for feature selection, effectively choosing a subset of important features and discarding the less important ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8536fc28-f7ae-45b4-b6a7-bbe24401d0e6",
   "metadata": {},
   "source": [
    "#### Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e251c1a2-e292-4469-932e-219487028123",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression has a built-in feature that can help handle multicollinearity in the input features. Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, making it challenging to identify the individual contribution of each variable to the dependent variable.\n",
    "\n",
    "it tends to shrink some coefficients exactly to zero, effectively performing feature selection. In the context of multicollinearity, this means that Lasso Regression can automatically choose one of the correlated variables and set the coefficients of the others to zero.\n",
    "\n",
    "By doing so, Lasso effectively selects a subset of features and discards the redundant or highly correlated ones. This property can be beneficial in situations where multicollinearity is present, as it helps to simplify the model and improve its interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09376bdb-cfdb-491d-9fe0-d6494ba62234",
   "metadata": {},
   "source": [
    "#### Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dd5b3d-3ad4-4fd1-99e4-f74a7dc51502",
   "metadata": {},
   "source": [
    "we use the process of cross validation here.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
