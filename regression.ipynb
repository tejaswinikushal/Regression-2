{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47b300ae-74e6-46c0-9e65-eae90d44b5bb",
   "metadata": {},
   "source": [
    "#### Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a32f0b8-243a-4f55-ac93-36928628eef1",
   "metadata": {},
   "source": [
    "#### R_Squared: \n",
    "it assesses the goodness of fit of the model to the observed data. The R-squared value ranges from 0 to 1, where:\n",
    "\n",
    "0 indicates that the model does not explain any variability in the dependent variable.\n",
    "\n",
    "1 indicates that the model perfectly explains the variability in the dependent variable.\n",
    "\n",
    "The formula for calculating R-squared is as follows:\n",
    "\n",
    "R Squared=1− SST/SSR\n",
    "\n",
    "where:\n",
    "\n",
    "1)SSR is the sum of squared residuals (the sum of the squared differences between the observed and predicted values of the dependent variable).\n",
    "\n",
    "2)SST is the total sum of squares (the sum of the squared differences between the observed values and the mean of the dependent variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b0a207-4358-4e39-9615-166ea68ebcee",
   "metadata": {},
   "source": [
    "#### Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e3b9a-3ef1-4f0e-bb56-97d780614953",
   "metadata": {},
   "source": [
    "Adjusted R-squared is an extended version of R squared where if we add a new feature it keeps on increasing the accuracy of the R squared,but w.r.t adjusted R Squared if a non use ful feature is added then the accuracy is not increased.\n",
    "\n",
    "Example :if we consider independent featues like Size_of_house,No_of_bedrooms and a dependent feature price->            now if we get R-Squared=85%\n",
    "         again if we add an independent feature like location -->now R-Squared=90% (i.e increases).So here drawback is although gender is not required to predict the orice of the house , \n",
    "\n",
    "formula:\n",
    "\n",
    "Adjusted R squared=1-(1-R^2)(N-1)/(N-p-1).\n",
    "\n",
    "where, N=number of data points\n",
    "       p=number of independent features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de20776-63ac-4aa0-91fb-4fbb5f257b15",
   "metadata": {},
   "source": [
    "#### Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2896a9d1-ba22-4286-aef6-5b318ec4d46b",
   "metadata": {},
   "source": [
    "1) If we add more features to a model them we use R_squared.\n",
    "\n",
    " 2. When dealing with complex models with a large number of predictors, adjusted R-squared provides a more nuanced evaluation, helping to identify whether the additional variables contribute meaningfully to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cec8a96-60ec-4d75-a837-7a0dc2028cdd",
   "metadata": {},
   "source": [
    "#### Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b80b3b3-29b1-4d19-9c07-9b766b393355",
   "metadata": {},
   "source": [
    "#### MSE:\n",
    "     MSE=summation(yi-y')^2/n\n",
    "\n",
    "#### Advantages:\n",
    "1.It is differentiable\n",
    "\n",
    "2.it has one local and one global minima\n",
    "#### Disadvantages:\n",
    "1.not robust to outliers.\n",
    "\n",
    "2.It is not in the same unit\n",
    "\n",
    "#### MAE:\n",
    "     MAE=(1/n)(summation(mod(yi-y')))\n",
    "     \n",
    "#### Advantages:\n",
    "1.since we are not squaring at thsat moment best fit curve will be small\n",
    "2.it will be in same unit.\n",
    "\n",
    "#### disadvantege:\n",
    "1.convergence takes time.\n",
    "\n",
    "#### RMSE:\n",
    "       RMSE=sqrt(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b9d9a8-2115-499c-b260-df39944ff339",
   "metadata": {},
   "source": [
    "#### Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efbaa98-b3c2-4e5c-9621-bb255cca411f",
   "metadata": {},
   "source": [
    "#### MSE :\n",
    "#### Advantages:\n",
    "1.It is differentiable\n",
    "\n",
    "2.it has one local and one global minima\n",
    "#### Disadvantages:\n",
    "1.not robust to outliers.\n",
    "\n",
    "2.It is not in the same unit\n",
    "\n",
    "#### Advantages:\n",
    "1.since we are not squaring at thsat moment best fit curve will be small\n",
    "2.it will be in same unit.\n",
    "\n",
    "#### disadvantege:\n",
    "1.convergence takes time.\n",
    "\n",
    "#### RMSE:\n",
    "#### Advantages:\n",
    "1.same unit\n",
    "2.differentiable\n",
    "\n",
    "#### Disadvantage:\n",
    "1.not robust to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f195d55-adb3-426c-a28f-199bc0209926",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?\n",
    "\n",
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate.\n",
    "\n",
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis.\n",
    "\n",
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fabbaf-59c1-49ef-ac29-410136627bc0",
   "metadata": {},
   "source": [
    "#### Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf61ffe8-c7aa-4672-a260-cf41ad635289",
   "metadata": {},
   "source": [
    "Lasso regularization, also known as L1 regularization, is a technique used in linear regression and machine learning to prevent overfitting and encourage the model to be more sparse. The regularization term is added to the cost function, and it penalizes the absolute values of the coefficients.\n",
    "\n",
    "The key difference between Lasso and Ridge regularization (L2 regularization) lies in the penalty term. While Lasso penalizes the absolute values of the coefficient,Ridge penalizes the squared values of the coefficients .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d6fb7b-2baf-4b05-a795-3bee7e61c0e0",
   "metadata": {},
   "source": [
    "#### Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986c60e4-53aa-48e8-b59a-6b5482d97027",
   "metadata": {},
   "source": [
    "Regularized linear models help prevent overfitting in machine learning by adding a penalty term to the linear regression cost function. The penalty term discourages the model from fitting the training data too closely and, as a result, helps to generalize better to unseen data. Two common types of regularization used in linear models are Ridge regularization (L2 regularization) and Lasso regularization (L1 regularization).\n",
    " \n",
    " Example: Ridge Regression (L2 Regularization) in Python:\n",
    "\n",
    "Let's consider a simple example using Ridge regression from the scikit-learn library in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65449a13-cd39-473f-8126-91b458c1c893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (Lasso Regression): 2898.368019282879\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Load the diabetes dataset\n",
    "data = load_diabetes()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features (important for Lasso regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply Lasso regression with a regularization parameter (alpha)\n",
    "alpha = 0.01\n",
    "lasso_model = Lasso(alpha=alpha)\n",
    "lasso_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = lasso_model.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (Lasso Regression): {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c67b13-c011-44e7-82c3-41837e689176",
   "metadata": {},
   "source": [
    "#### Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ae25a6-0fa0-4ab8-b1e3-aaa334f1cc8a",
   "metadata": {},
   "source": [
    "Not Suitable for Non-Linear Relationships\n",
    "\n",
    "Loss of Interpretability\n",
    "\n",
    "Feature scaling dependency\n",
    "\n",
    "Not Suitable for Non-Linear Relationships\n",
    "\n",
    "Sensitive to Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c45bf0c-b3bb-4c85-8c04-f17147448c2a",
   "metadata": {},
   "source": [
    "#### Q10. You are comparing the performance of two regularized linear models using different types ofregularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fcd814-fd12-4af6-87f2-acf9eed7acb4",
   "metadata": {},
   "source": [
    "The choice between Ridge regularization (L2 regularization) and Lasso regularization (L1 regularization) depends on the specific characteristics of your data and the goals of your model. Let's consider the given scenario:\n",
    "\n",
    "Model A (Ridge Regularization with λ=0.1):\n",
    "\n",
    "Ridge regularization adds a penalty term proportional to the squared values of the coefficients. It tends to shrink all coefficients towards zero but does not result in exact sparsity (i.e., setting coefficients to exactly zero).\n",
    "With a regularization parameter of 0.1, Ridge is providing a moderate amount of regularization.\n",
    "Model B (Lasso Regularization with λ=0.5):\n",
    "\n",
    "Lasso regularization, on the other hand, adds a penalty term proportional to the absolute values of the coefficients. It has the property of inducing sparsity in the model, often resulting in some coefficients being exactly zero.\n",
    "With a regularization parameter of 0.5, Lasso is providing a stronger regularization compared to Ridge.\n",
    "\n",
    "#### Choosing the Better Performer:\n",
    "\n",
    "1.If interpretability of the model is crucial and you want to understand the contribution of all features, Ridge might be a better choice. Ridge tends to shrink coefficients, but they are unlikely to become exactly zero.\n",
    "\n",
    "2.If feature selection is important, and you believe that some features can be entirely excluded from the model, Lasso might be preferable. Lasso tends to drive some coefficients to exactly zero, effectively performing feature selection.\n",
    "\n",
    "#### Trade-offs and Limitations:\n",
    "\n",
    "#### Bias-Variance Trade-off:\n",
    "\n",
    "Ridge tends to introduce a small amount of bias but often reduces variance, making it suitable when there is multicollinearity between features.\n",
    "Lasso, with its feature selection property, can increase bias but might be more robust in the presence of irrelevant features.\n",
    "\n",
    "#### Sensitivity to Feature Scaling:\n",
    "\n",
    "Lasso is more sensitive to the scale of features compared to Ridge. It's important to standardize or normalize features before applying Lasso to ensure fair treatment.\n",
    "\n",
    "#### Number of Features:\n",
    "\n",
    "If the dataset has a large number of features, Lasso's ability to perform feature selection can be advantageous. Ridge may not automatically exclude features from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14489a61-fc48-468d-bc9b-7a43e91965f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
